{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/besto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/besto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/besto/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/besto/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Elsevier Developer Portal API Key\n",
    "ELS_API_KEY = os.getenv(\"ELS_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling Journal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we gonna create dataset that collected by using Elsevier API.\n",
    "\n",
    "In the `search_results` directory, the search results of 1000 articles from the Future Generation Computer Systems journal will be stored in JSON file format. The search results will include the `Publisher Item Identifier (PII)`, which will be used to find the `title`, `abstract`, and `keywords`.\n",
    "\n",
    "steps\n",
    "1. read each file in search_results directory\n",
    "2. extract `PII` from each result\n",
    "3. find the `title`, `abstract`, and `keywords` by using Article Retrieval API with `PII`\n",
    "4. create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_data(pii: str) -> list[str, str | list[str]]:\n",
    "    url = f\"https://api.elsevier.com/content/article/pii/{pii}\"\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"X-ELS-APIKey\": ELS_API_KEY\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"{response.status_code}: {response.text}\")\n",
    "\n",
    "    resp_json = response.json()\n",
    "    coredata = resp_json[\"full-text-retrieval-response\"][\"coredata\"]\n",
    "    title, abstract = coredata[\"dc:title\"], coredata[\"dc:description\"]\n",
    "    if \"dcterms:subject\" in coredata:\n",
    "        keywords = [ sub[\"$\"] for sub in coredata[\"dcterms:subject\"] ]\n",
    "    else:\n",
    "        keywords = []\n",
    "    \n",
    "    return [title, abstract, keywords]\n",
    "\n",
    "def create_dataset_file(filename: str, headers: list[str], data_list: list[list[str | list[str]]]) -> None:\n",
    "    with open(filename, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(headers)\n",
    "        writer.writerows(data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data_list = []\n",
    "    for root, dirs, files in os.walk(\"./search_results\"):\n",
    "        for file in files:\n",
    "            filepath = root + \"/\" + file\n",
    "\n",
    "            with open(filepath) as f:\n",
    "                search_results = json.load(f)\n",
    "\n",
    "            for result in search_results[\"results\"]:\n",
    "                pii = result[\"pii\"]\n",
    "                data = get_article_data(pii)\n",
    "                data_list.append(data)\n",
    "\n",
    "            time.sleep(10) # prevent a rate limit error\n",
    "\n",
    "    headers = [\"title\", \"abstract\", \"keywords\"]\n",
    "    create_dataset_file(filename=\"dataset.csv\", headers=headers, data_list=data_list)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>On the improvement of wireless mesh sensor net...</td>\n",
       "      <td>Wireless Mesh Sensor Networks (WMSNs) have rec...</td>\n",
       "      <td>['Wireless Mesh Sensor Networks', 'Hidden term...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Software Tools and Techniques for Big Data Com...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Editorial Board</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Contents</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dynamic counter-measures for risk-based access...</td>\n",
       "      <td>\\n                  Risk-based access control ...</td>\n",
       "      <td>['ISO 27001', 'ISMS', 'Risk management', 'Acce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  On the improvement of wireless mesh sensor net...   \n",
       "1  Software Tools and Techniques for Big Data Com...   \n",
       "2                                   Editorial Board    \n",
       "3                                          Contents    \n",
       "4  Dynamic counter-measures for risk-based access...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Wireless Mesh Sensor Networks (WMSNs) have rec...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  \\n                  Risk-based access control ...   \n",
       "\n",
       "                                            keywords  \n",
       "0  ['Wireless Mesh Sensor Networks', 'Hidden term...  \n",
       "1                                                 []  \n",
       "2                                                 []  \n",
       "3                                                 []  \n",
       "4  ['ISO 27001', 'ISMS', 'Risk management', 'Acce...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title         0\n",
       "abstract    136\n",
       "keywords      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title       0\n",
       "abstract    0\n",
       "keywords    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(864, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def text_preprocessing(text: str) -> list[str]:\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word.lower() for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    pos_tags = pos_tag(filtered_tokens)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
    "\n",
    "\n",
    "    nouns = [word for word, pos in pos_tag(lemmatized_tokens) if pos in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
    "\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyword_dicts(s: pd.Series) -> dict[str, int]:\n",
    "    keyword_dict = {}\n",
    "    for keywords_str in s:\n",
    "        keyword_list = ast.literal_eval(keywords_str)\n",
    "        for keyword in keyword_list:\n",
    "            if keyword not in keyword_dict:\n",
    "                keyword_dict[keyword] = 1\n",
    "            else:\n",
    "                keyword_dict[keyword] += 1\n",
    "    return dict(sorted(keyword_dict.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_dict = get_keyword_dicts(df[\"keywords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "top frequency 1-10\n",
      "------------------------------\n",
      "Cloud computing 156\n",
      "Big data 33\n",
      "Internet of Things 31\n",
      "Security 26\n",
      "Scheduling 25\n",
      "Scientific workflows 21\n",
      "MapReduce 19\n",
      "Resource management 19\n",
      "Energy efficiency 18\n",
      "Virtualization 16\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*30)\n",
    "print(\"top frequency 1-10\")\n",
    "print(\"-\"*30)\n",
    "top10_keyword_frequency = list(keyword_dict.keys())[:10]\n",
    "for key in top10_keyword_frequency:\n",
    "    print(key, keyword_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "top frequency 1-20\n",
      "------------------------------\n",
      "Cloud computing 156\n",
      "Big data 33\n",
      "Internet of Things 31\n",
      "Security 26\n",
      "Scheduling 25\n",
      "Scientific workflows 21\n",
      "MapReduce 19\n",
      "Resource management 19\n",
      "Energy efficiency 18\n",
      "Virtualization 16\n",
      "Cloud 16\n",
      "Privacy 15\n",
      "Cloud storage 14\n",
      "Hadoop 13\n",
      "Performance evaluation 13\n",
      "Provenance 13\n",
      "Clustering 13\n",
      "Task scheduling 12\n",
      "Resource allocation 12\n",
      "Quality of service 12\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*30)\n",
    "print(\"top frequency 1-20\")\n",
    "print(\"-\"*30)\n",
    "top20_keyword_frequency = list(keyword_dict.keys())[:20]\n",
    "for key in top20_keyword_frequency:\n",
    "    print(key, keyword_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "top frequency 1-30\n",
      "------------------------------\n",
      "Cloud computing 156\n",
      "Big data 33\n",
      "Internet of Things 31\n",
      "Security 26\n",
      "Scheduling 25\n",
      "Scientific workflows 21\n",
      "MapReduce 19\n",
      "Resource management 19\n",
      "Energy efficiency 18\n",
      "Virtualization 16\n",
      "Cloud 16\n",
      "Privacy 15\n",
      "Cloud storage 14\n",
      "Hadoop 13\n",
      "Performance evaluation 13\n",
      "Provenance 13\n",
      "Clustering 13\n",
      "Task scheduling 12\n",
      "Resource allocation 12\n",
      "Quality of service 12\n",
      "Data mining 12\n",
      "QoS 11\n",
      "Wireless sensor networks 11\n",
      "Big Data 11\n",
      "Semantic web 11\n",
      "IoT 11\n",
      "Load balancing 10\n",
      "Simulation 10\n",
      "Game theory 10\n",
      "Distributed computing 10\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*30)\n",
    "print(\"top frequency 1-30\")\n",
    "print(\"-\"*30)\n",
    "top30_keyword_frequency = list(keyword_dict.keys())[:30]\n",
    "for key in top30_keyword_frequency:\n",
    "    print(key, keyword_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.024*\"service\" + 0.023*\"cloud\" + 0.023*\"application\" + 0.016*\"resource\" + 0.015*\"system\" + 0.015*\"network\" + 0.012*\"use\" + 0.012*\"paper\" + 0.010*\"model\" + 0.009*\"management\"')\n",
      "(1, '0.018*\"data\" + 0.015*\"system\" + 0.013*\"iot\" + 0.012*\"query\" + 0.010*\"event\" + 0.009*\"entity\" + 0.009*\"network\" + 0.009*\"use\" + 0.008*\"paper\" + 0.008*\"method\"')\n",
      "(2, '0.042*\"scheme\" + 0.021*\"data\" + 0.019*\"security\" + 0.017*\"paper\" + 0.014*\"encryption\" + 0.012*\"attack\" + 0.011*\"use\" + 0.011*\"user\" + 0.011*\"network\" + 0.010*\"secure\"')\n",
      "(3, '0.065*\"data\" + 0.019*\"system\" + 0.015*\"process\" + 0.013*\"city\" + 0.011*\"paper\" + 0.011*\"application\" + 0.011*\"service\" + 0.010*\"cloud\" + 0.009*\"use\" + 0.009*\"approach\"')\n",
      "(4, '0.043*\"data\" + 0.024*\"model\" + 0.023*\"resource\" + 0.012*\"use\" + 0.009*\"paper\" + 0.008*\"result\" + 0.008*\"machine\" + 0.008*\"problem\" + 0.008*\"network\" + 0.008*\"analysis\"')\n",
      "(5, '0.017*\"service\" + 0.016*\"system\" + 0.016*\"approach\" + 0.014*\"model\" + 0.014*\"result\" + 0.013*\"application\" + 0.013*\"data\" + 0.012*\"performance\" + 0.011*\"base\" + 0.011*\"time\"')\n",
      "(6, '0.019*\"application\" + 0.019*\"performance\" + 0.018*\"service\" + 0.018*\"model\" + 0.013*\"system\" + 0.012*\"data\" + 0.010*\"use\" + 0.009*\"paper\" + 0.009*\"time\" + 0.008*\"base\"')\n",
      "(7, '0.029*\"resource\" + 0.022*\"schedule\" + 0.022*\"energy\" + 0.020*\"task\" + 0.016*\"algorithm\" + 0.012*\"cloud\" + 0.011*\"network\" + 0.011*\"use\" + 0.011*\"time\" + 0.011*\"cluster\"')\n",
      "(8, '0.045*\"data\" + 0.031*\"system\" + 0.012*\"performance\" + 0.011*\"use\" + 0.010*\"model\" + 0.010*\"paper\" + 0.009*\"storage\" + 0.009*\"time\" + 0.009*\"cloud\" + 0.008*\"base\"')\n",
      "(9, '0.023*\"cloud\" + 0.015*\"consolidation\" + 0.012*\"pm\" + 0.011*\"vm\" + 0.011*\"approach\" + 0.011*\"paper\" + 0.010*\"number\" + 0.010*\"trust\" + 0.010*\"compute\" + 0.009*\"method\"')\n"
     ]
    }
   ],
   "source": [
    "texts = [text_preprocessing(abs) for abs in df[\"abstract\"]]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "lda_model = LdaModel(corpus, num_topics=10, id2word=dictionary, passes=10)\n",
    "topics = lda_model.print_topics()\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
